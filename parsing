from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import sqlite3

# Подключение к базе данных
conn = sqlite3.connect('products.db')
cursor = conn.cursor()


def scroll_to_load(driver):
    """Функция для плавного скроллинга страницы вниз, чтобы загрузить все элементы."""
    scroll_pause_time = 1  # Время ожидания после каждой прокрутки
    last_height = driver.execute_script("return document.body.scrollHeight")

    while True:
        # Прокручиваем страницу вниз
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(scroll_pause_time)

        # Получаем новую высоту страницы
        new_height = driver.execute_script("return document.body.scrollHeight")

        # Если высота больше не меняется, значит мы дошли до конца
        if new_height == last_height:
            break
        last_height = new_height


def cards_parsing(soup):
    """Функция для парсинга карточек продуктов."""
    data = []

    # Ищем все карточки продуктов
    cards = soup.find_all('div', class_='CatalogProduct__content mediaText')

    for card in cards:
        # Извлекаем название товара и ссылку
        title_div = card.find('div', class_='CatalogProduct__title')
        title_link = title_div.find('a') if title_div else None
        name = title_link.text.strip() if title_link else 'Название отсутствует'
        link = title_link['href'] if title_link else 'Ссылка отсутствует'
        # print(soup.find_all('div', class_='CatalogProduct__imageBox mediaBox').find_all("div", class_='CatalogProduct__image-link'))

        # image_url ="мяу"
        # picture_tag = card.find('div', class_='CatalogProduct CatalogRow__cell isLight')  # Ищем тег <picture>
        # print(picture_tag)
        # if picture_tag:
        #     source_tag = picture_tag.find('source')  # Ищем первый тег <source>
        #     if source_tag and 'srcset' in source_tag.attrs:
        #         # Берем первую ссылку из srcset
        #         image_url = source_tag['srcset'].split(' ')[0]
        #
        #
        # break

        # Извлекаем цену товара
        price_div = card.find('div', class_='CatalogProduct__price')
        price = price_div.text.strip() if price_div else 'Цена отсутствует'

        # Добавляем собранные данные в список
        data.append({
            'name': name,
            'link': link,
            'price': price
        })

        # Вставка записи в таблицу
        cursor.execute('''
        INSERT INTO products (name, price, type, color, photo, link)
        VALUES (?, ?, ?, ?, ?, ?)
        ''', (name, price, None, None, None, "https://lime-shop.com/" + link))

        # Сохранение изменений и закрытие соединения
        conn.commit()

    return data


def do_parsing():
    driver = webdriver.Safari()
    driver.maximize_window()

    try:
        driver.get('https://lime-shop.com/ru_ru/catalog/men_view_all')

        # Ожидаем появления первого элемента на странице
        WebDriverWait(driver, 10).until(
            EC.visibility_of_element_located((By.CLASS_NAME, 'CatalogProduct__content'))
        )

        # time.sleep(10)

        # Прокручиваем страницу, чтобы загрузить все элементы
        scroll_to_load(driver)

        # Получаем HTML-код страницы после загрузки всех элементов
        soup = BeautifulSoup(driver.page_source, 'lxml')

        data = cards_parsing(soup)
        # for item in data:
        #     print(item)

    except Exception as e:
        print(f"Произошла ошибка: {e}")

    finally:
        # Закрываем браузер
        driver.quit()

        conn.close()
