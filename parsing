from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import sqlite3

# Подключение к базе данных
conn = sqlite3.connect('products.db')
cursor = conn.cursor()


def scroll_to_load(driver):
    """Функция для плавного скроллинга страницы вниз, чтобы загрузить все элементы."""
    scroll_pause_time = 1  # Время ожидания после каждой прокрутки
    last_height = driver.execute_script("return document.body.scrollHeight")

    while True:
        # Прокручиваем страницу вниз
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(scroll_pause_time)

        # Получаем новую высоту страницы
        new_height = driver.execute_script("return document.body.scrollHeight")

        # Если высота больше не меняется, значит мы дошли до конца
        if new_height == last_height:
            break
        last_height = new_height


def get_photos(driver, link):
    driver.get(link)
    time.sleep(3)
    images = driver.find_elements(By.CSS_SELECTOR, "img")
    image_urls = [img.get_attribute("src") for img in images]

    return image_urls


def get_lime_photo_for_db(driver):
    cursor.execute("SELECT id, link FROM products")
    rows = cursor.fetchall()
    for row in rows:
        product_id, link = row
        print(link)
        if link:
            photo = ",".join(get_photos(driver, link))
            cursor.execute("UPDATE products SET photo = ? WHERE id = ?", (photo, product_id))
            conn.commit()


def cards_parsing(soup, driver):
    """Функция для парсинга карточек продуктов."""
    data = []

    # Ищем все карточки продуктов
    cards = soup.find_all('div', class_='CatalogProduct__content mediaText')

    for card in cards:
        # Извлекаем название товара и ссылку
        title_div = card.find('div', class_='CatalogProduct__title')
        title_link = title_div.find('a') if title_div else None
        name = title_link.text.strip() if title_link else 'Название отсутствует'
        link = "https://lime-shop.com" + title_link['href'] if title_link else 'Ссылка отсутствует'

        # Извлекаем цену товара
        price_div = card.find('div', class_='CatalogProduct__price')
        price = price_div.text.strip() if price_div else 'Цена отсутствует'

        # Добавляем собранные данные в список
        data.append({
            'name': name,
            'link': link,
            'price': price
        })

        # Вставка записи в таблицу
        cursor.execute('''
        INSERT INTO products (name, price, type, color, photo, link)
        VALUES (?, ?, ?, ?, ?, ?)
        ''', (name, price, None, None, None, link))

        # Сохранение изменений и закрытие соединения
        conn.commit()

    return data


def main():
    driver = webdriver.Safari()
    driver.maximize_window()

    try:
        driver.get('https://lime-shop.com/ru_ru/catalog/men_view_all')

        # Ожидаем появления первого элемента на странице
        WebDriverWait(driver, 10).until(
            EC.visibility_of_element_located((By.CLASS_NAME, 'CatalogProduct__content'))
        )

        # time.sleep(10)
        scroll_to_load(driver)
        soup = BeautifulSoup(driver.page_source, 'lxml')

        data = cards_parsing(soup, driver)
        # for item in data:
        #     print(item)

        get_lime_photo_for_db(driver)

    except Exception as e:
        print(f"Произошла ошибка: {e}")


    finally:
        # Закрываем браузер
        driver.quit()
        conn.close()


if __name__ == "__main__":
    main()
